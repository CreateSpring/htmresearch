{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Learn four objects with three sensors\n",
    "\n",
    "This visualization shows:\n",
    "\n",
    "- 3 cortical columns\n",
    "- A \"world\" that contains 4 objects\n",
    "- An animal in the world. It has a body and 3 sensors.\n",
    "\n",
    "This visualization shows a body, 3 sensors, and 4 objects.\n",
    "\n",
    "In the simulation below, the animal moves its sensors over 4 objects. It does 1-shot learning. (1-shot learning is not necessary, it's just good for this demo.)\n",
    "\n",
    "To learn, the animal activates a random \"body relative to specific object\", then proceeds to move its sensors over the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import htmresearchviz0.IPython_support as viz\n",
    "viz.init_notebook_mode()\n",
    "with open(\"logs/100-cells-learn.log\", \"r\") as fileIn:\n",
    "    viz.printMultiColumnInferenceRecording(fileIn.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every sensation, a feature-location pair is predicted after 1 timestep.\n",
    "\n",
    "These feature-location pairs are associated with 4 active cells per module. Because these discrete neurons are handling continuous space, the \"sensor relative to specific object\" is always at least 4 cells, because the \"sensor relative to body\" is somewhat ambiguous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Infer the objects\n",
    "\n",
    "Now the animal has moved elsewhere in the world. It's going to experience this objects at an egocentric location that it has never experienced these objects at before.\n",
    "\n",
    "It touches every object twice.\n",
    "\n",
    "In this visualization, every timestep is divided into two parts: \"?\" and \"!\", which you can pronounce as \"predict\" and \"sense\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"logs/100-cells-infer.log\", \"r\") as fileIn:\n",
    "    viz.printMultiColumnInferenceRecording(fileIn.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works!\n",
    "\n",
    "These unions of locations are often huge. This is because:\n",
    "\n",
    "- Every feature-location pair activates 4 \"sensor relative to specific object\" cells\n",
    "- Hence, every feature-location pair activates 9 \"body relative to specific object\".\n",
    "  - (Because, again, this egocentric location is represented by discrete neurons so it has some ambiguity)\n",
    "  \n",
    "It would be nice if we had a way to associate feature-location pairs with 1 cell per module, rather than 4. That would dramatically reduce the union size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
